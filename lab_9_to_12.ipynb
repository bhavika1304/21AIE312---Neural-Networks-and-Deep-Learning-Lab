{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9267686,"sourceType":"datasetVersion","datasetId":5607234}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lab 9","metadata":{}},{"cell_type":"markdown","source":"Question A1. Continue your project work from last week’s exercise. Experiment with various optimizers and report your results with loss values, accuracies, precisions, recalls and F-scores.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:19.147744Z","iopub.execute_input":"2025-04-11T13:26:19.148137Z","iopub.status.idle":"2025-04-11T13:26:38.600189Z","shell.execute_reply.started":"2025-04-11T13:26:19.148110Z","shell.execute_reply":"2025-04-11T13:26:38.599139Z"}},"outputs":[{"name":"stderr","text":"2025-04-11 13:26:21.200596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744377981.477242      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744377981.551166      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force TensorFlow to use CPU only\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   # Suppress TensorFlow warnings and errors\n\n# ----------------------------------------\n# ✅ Paths and Constants\n# ----------------------------------------\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 10\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.601707Z","iopub.execute_input":"2025-04-11T13:26:38.602256Z","iopub.status.idle":"2025-04-11T13:26:38.608826Z","shell.execute_reply.started":"2025-04-11T13:26:38.602232Z","shell.execute_reply":"2025-04-11T13:26:38.607380Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Load Data using ImageDataGenerator\n# ----------------------------------------\ndef load_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    \n    train_data = datagen.flow_from_directory(\n        train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n        class_mode='categorical', seed=SEED)\n\n    test_data = datagen.flow_from_directory(\n        test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n        class_mode='categorical', shuffle=False, seed=SEED)\n    \n    return train_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.610165Z","iopub.execute_input":"2025-04-11T13:26:38.610613Z","iopub.status.idle":"2025-04-11T13:26:38.656159Z","shell.execute_reply.started":"2025-04-11T13:26:38.610579Z","shell.execute_reply":"2025-04-11T13:26:38.655080Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Define CNN Model Architecture\n# ----------------------------------------\ndef build_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),  # Add this to fix warning\n        Conv2D(32, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.658564Z","iopub.execute_input":"2025-04-11T13:26:38.658907Z","iopub.status.idle":"2025-04-11T13:26:38.679692Z","shell.execute_reply.started":"2025-04-11T13:26:38.658862Z","shell.execute_reply":"2025-04-11T13:26:38.678668Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Train Model with Given Optimizer\n# ----------------------------------------\ndef train_with_optimizer(optimizer_name, train_data, test_data):\n    print(f\"\\n🔧 Training with optimizer: {optimizer_name.upper()}\\n\")\n\n    input_shape = IMG_SIZE + (3,)\n    num_classes = train_data.num_classes\n    model = build_cnn_model(input_shape, num_classes)\n\n    # Select optimizer\n    optimizer_dict = {\n        'adam': tf.keras.optimizers.Adam(),\n        'sgd': tf.keras.optimizers.SGD(),\n        'rmsprop': tf.keras.optimizers.RMSprop()\n    }\n\n    model.compile(optimizer=optimizer_dict[optimizer_name],\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    # Train model\n    history = model.fit(train_data, epochs=EPOCHS,\n                        validation_data=test_data, verbose=1)\n\n    # Predict and evaluate\n    predictions = model.predict(test_data)\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = test_data.classes\n    class_labels = list(test_data.class_indices.keys())\n\n    # Print classification report\n    print(classification_report(y_true, y_pred, target_names=class_labels))\n\n    return history\n\n# ----------------------------------------\n# ✅ Main Execution: Try All Optimizers\n# ----------------------------------------\nif __name__ == \"__main__\":\n    train_data, test_data = load_data()\n    for optimizer in ['adam', 'sgd', 'rmsprop']:\n        train_with_optimizer(optimizer, train_data, test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.680715Z","iopub.execute_input":"2025-04-11T13:26:38.680994Z","iopub.status.idle":"2025-04-11T14:56:46.593103Z","shell.execute_reply.started":"2025-04-11T13:26:38.680973Z","shell.execute_reply":"2025-04-11T14:56:46.592087Z"}},"outputs":[{"name":"stdout","text":"Found 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\n\n🔧 Training with optimizer: ADAM\n\n","output_type":"stream"},{"name":"stderr","text":"2025-04-11 13:26:41.352395: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 2s/step - accuracy: 0.3575 - loss: 2.2042 - val_accuracy: 0.8955 - val_loss: 0.4036\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.8412 - loss: 0.4602 - val_accuracy: 0.9763 - val_loss: 0.1384\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9274 - loss: 0.2156 - val_accuracy: 0.9914 - val_loss: 0.0531\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9446 - loss: 0.1553 - val_accuracy: 0.9989 - val_loss: 0.0139\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 2s/step - accuracy: 0.9781 - loss: 0.0650 - val_accuracy: 1.0000 - val_loss: 0.0067\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9782 - loss: 0.0564 - val_accuracy: 1.0000 - val_loss: 0.0019\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9895 - loss: 0.0318 - val_accuracy: 1.0000 - val_loss: 0.0035\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9894 - loss: 0.0423 - val_accuracy: 1.0000 - val_loss: 0.0049\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9868 - loss: 0.0402 - val_accuracy: 1.0000 - val_loss: 0.0024\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9926 - loss: 0.0328 - val_accuracy: 1.0000 - val_loss: 7.0992e-04\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 984ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n\n🔧 Training with optimizer: SGD\n\nEpoch 1/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 0.2823 - loss: 1.4560 - val_accuracy: 0.3060 - val_loss: 1.3713\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.3191 - loss: 1.3639 - val_accuracy: 0.3060 - val_loss: 1.3632\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.3297 - loss: 1.3578 - val_accuracy: 0.2748 - val_loss: 1.3479\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.3926 - loss: 1.3354 - val_accuracy: 0.7608 - val_loss: 1.2911\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.4616 - loss: 1.2710 - val_accuracy: 0.6261 - val_loss: 1.1449\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.5649 - loss: 1.1277 - val_accuracy: 0.4892 - val_loss: 1.0628\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.6784 - loss: 0.8891 - val_accuracy: 0.7295 - val_loss: 0.7851\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.7443 - loss: 0.7260 - val_accuracy: 0.8513 - val_loss: 0.5288\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8189 - loss: 0.5376 - val_accuracy: 0.8976 - val_loss: 0.4278\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.8291 - loss: 0.4696 - val_accuracy: 0.9256 - val_loss: 0.2503\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.91      0.85      0.88       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.99      0.82      0.89       233\n                          Normal Person ECG Images (284x12=3408)       0.85      1.00      0.92       284\n\n                                                        accuracy                           0.93       928\n                                                       macro avg       0.94      0.92      0.92       928\n                                                    weighted avg       0.93      0.93      0.92       928\n\n\n🔧 Training with optimizer: RMSPROP\n\nEpoch 1/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.3637 - loss: 4.2585 - val_accuracy: 0.2575 - val_loss: 2.4308\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.6776 - loss: 0.8934 - val_accuracy: 0.9817 - val_loss: 0.1185\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.8957 - loss: 0.2804 - val_accuracy: 0.9946 - val_loss: 0.0326\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.9673 - loss: 0.0981 - val_accuracy: 1.0000 - val_loss: 0.0137\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9842 - loss: 0.0483 - val_accuracy: 1.0000 - val_loss: 0.0013\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.0382 - val_accuracy: 1.0000 - val_loss: 7.1439e-04\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.9956 - loss: 0.0161 - val_accuracy: 1.0000 - val_loss: 0.0015\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.0361 - val_accuracy: 1.0000 - val_loss: 4.3497e-05\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.9963 - loss: 0.0100 - val_accuracy: 1.0000 - val_loss: 2.9650e-05\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9967 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 2.3899e-06\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Lab 10","metadata":{}},{"cell_type":"markdown","source":"Question A1. Continue your project work from last week’s exercise. Experiment with various regularization & optimization techniques and report your results with loss values, accuracies, precisions, recalls and F-scores.  ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.594130Z","iopub.execute_input":"2025-04-11T14:56:46.594491Z","iopub.status.idle":"2025-04-11T14:56:46.604544Z","shell.execute_reply.started":"2025-04-11T14:56:46.594463Z","shell.execute_reply":"2025-04-11T14:56:46.603374Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Environment setup\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Constants\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.605705Z","iopub.execute_input":"2025-04-11T14:56:46.606078Z","iopub.status.idle":"2025-04-11T14:56:46.627450Z","shell.execute_reply.started":"2025-04-11T14:56:46.606045Z","shell.execute_reply":"2025-04-11T14:56:46.626274Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load data\ndef load_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    train = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n    return train, test\n\n# CNN with L2 and Dropout\ndef build_regularized_cnn(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),\n        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.628550Z","iopub.execute_input":"2025-04-11T14:56:46.629277Z","iopub.status.idle":"2025-04-11T14:56:46.658058Z","shell.execute_reply.started":"2025-04-11T14:56:46.629240Z","shell.execute_reply":"2025-04-11T14:56:46.656947Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Train & Evaluate\ndef train_regularized_model(optimizer_name):\n    train_data, test_data = load_data()\n    input_shape = IMG_SIZE + (3,)\n    num_classes = train_data.num_classes\n    model = build_regularized_cnn(input_shape, num_classes)\n\n    optimizers = {\n        'adam': tf.keras.optimizers.Adam(),\n        'rmsprop': tf.keras.optimizers.RMSprop()\n    }\n\n    model.compile(optimizer=optimizers[optimizer_name],\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    model.fit(train_data, epochs=EPOCHS, validation_data=test_data, verbose=1)\n    \n    predictions = model.predict(test_data)\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = test_data.classes\n    class_labels = list(test_data.class_indices.keys())\n\n    print(classification_report(y_true, y_pred, target_names=class_labels))\n\n# Run\nif __name__ == '__main__':\n    for opt in ['adam', 'rmsprop']:\n        print(f\"\\n🔧 Training with {opt.upper()} (with L2 & Dropout)\\n\")\n        train_regularized_model(opt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.659160Z","iopub.execute_input":"2025-04-11T14:56:46.660833Z","iopub.status.idle":"2025-04-11T15:59:33.975270Z","shell.execute_reply.started":"2025-04-11T14:56:46.660795Z","shell.execute_reply":"2025-04-11T15:59:33.974194Z"}},"outputs":[{"name":"stdout","text":"\n🔧 Training with ADAM (with L2 & Dropout)\n\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 2s/step - accuracy: 0.2913 - loss: 2.7330 - val_accuracy: 0.3782 - val_loss: 1.4806\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.4564 - loss: 1.3592 - val_accuracy: 0.8244 - val_loss: 0.7951\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 2s/step - accuracy: 0.7417 - loss: 0.7851 - val_accuracy: 0.9310 - val_loss: 0.4356\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 2s/step - accuracy: 0.8324 - loss: 0.5690 - val_accuracy: 0.9741 - val_loss: 0.2437\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 2s/step - accuracy: 0.8621 - loss: 0.4460 - val_accuracy: 0.9881 - val_loss: 0.2119\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9018 - loss: 0.3836 - val_accuracy: 0.9935 - val_loss: 0.1763\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.9157 - loss: 0.3321 - val_accuracy: 0.9968 - val_loss: 0.1441\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.9192 - loss: 0.3084 - val_accuracy: 0.9849 - val_loss: 0.1518\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9311 - loss: 0.2846 - val_accuracy: 0.9978 - val_loss: 0.1317\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.9405 - loss: 0.2725 - val_accuracy: 1.0000 - val_loss: 0.1188\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 988ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n\n🔧 Training with RMSPROP (with L2 & Dropout)\n\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2s/step - accuracy: 0.3149 - loss: 4.1873 - val_accuracy: 0.5162 - val_loss: 1.3797\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 2s/step - accuracy: 0.5657 - loss: 1.2095 - val_accuracy: 0.8567 - val_loss: 0.5850\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.8356 - loss: 0.5794 - val_accuracy: 0.9267 - val_loss: 0.3315\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9107 - loss: 0.3412 - val_accuracy: 0.9871 - val_loss: 0.2289\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.9660 - loss: 0.2253 - val_accuracy: 0.9989 - val_loss: 0.1349\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 2s/step - accuracy: 0.9733 - loss: 0.1948 - val_accuracy: 0.9440 - val_loss: 0.2413\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 2s/step - accuracy: 0.9788 - loss: 0.1722 - val_accuracy: 0.9644 - val_loss: 0.1799\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 2s/step - accuracy: 0.9785 - loss: 0.1420 - val_accuracy: 1.0000 - val_loss: 0.0907\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 2s/step - accuracy: 0.9912 - loss: 0.1128 - val_accuracy: 1.0000 - val_loss: 0.0843\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 2s/step - accuracy: 0.9939 - loss: 0.1068 - val_accuracy: 1.0000 - val_loss: 0.0786\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Question A2. Study about RNN and learn to implement a basic RNN. Identify how to use RNN for your project work. ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Dense, Reshape, Input\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T16:47:49.194407Z","iopub.execute_input":"2025-04-11T16:47:49.194777Z","iopub.status.idle":"2025-04-11T16:48:07.348812Z","shell.execute_reply.started":"2025-04-11T16:47:49.194753Z","shell.execute_reply":"2025-04-11T16:48:07.347774Z"}},"outputs":[{"name":"stderr","text":"2025-04-11 16:47:51.054891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744390071.317355      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744390071.391210      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Environment config\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Config\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:13:50.579991Z","iopub.execute_input":"2025-04-11T17:13:50.580335Z","iopub.status.idle":"2025-04-11T17:13:50.585734Z","shell.execute_reply.started":"2025-04-11T17:13:50.580309Z","shell.execute_reply":"2025-04-11T17:13:50.584858Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load data\ndef load_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    train = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n    return train, test\n\n# Basic RNN model (reshape image to sequences)\ndef build_rnn(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),          # ✅ input_shape = (180, 540)\n        SimpleRNN(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:13:51.699212Z","iopub.execute_input":"2025-04-11T17:13:51.699560Z","iopub.status.idle":"2025-04-11T17:13:51.706331Z","shell.execute_reply.started":"2025-04-11T17:13:51.699537Z","shell.execute_reply":"2025-04-11T17:13:51.705195Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Train and evaluate RNN\ndef train_rnn_model():\n    train_data, test_data = load_data()\n    num_classes = train_data.num_classes\n\n    # Flatten images for RNN input\n    X_train, y_train = [], []\n    for i in range(len(train_data)):\n        x, y = train_data[i]\n        X_train.append(x.reshape(x.shape[0], IMG_SIZE[0], IMG_SIZE[1]*3))\n        y_train.append(y)\n        if len(X_train)*BATCH_SIZE >= train_data.samples:\n            break\n    X_train = np.concatenate(X_train)\n    y_train = np.concatenate(y_train)\n\n    X_test, y_test = [], []\n    for i in range(len(test_data)):\n        x, y = test_data[i]\n        X_test.append(x.reshape(x.shape[0], IMG_SIZE[0], IMG_SIZE[1]*3))\n        y_test.append(y)\n        if len(X_test)*BATCH_SIZE >= test_data.samples:\n            break\n    X_test = np.concatenate(X_test)\n    y_test = np.concatenate(y_test)\n\n    model = build_rnn((IMG_SIZE[0], IMG_SIZE[1]*3), num_classes)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1)\n\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    class_labels = list(test_data.class_indices.keys())\n    print(classification_report(y_true, y_pred, target_names=class_labels))\n\n# Run\nif __name__ == '__main__':\n    train_rnn_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:13:55.115059Z","iopub.execute_input":"2025-04-11T17:13:55.115392Z","iopub.status.idle":"2025-04-11T17:19:12.307929Z","shell.execute_reply.started":"2025-04-11T17:13:55.115366Z","shell.execute_reply":"2025-04-11T17:19:12.306873Z"}},"outputs":[{"name":"stdout","text":"Found 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 51ms/step - accuracy: 0.2910 - loss: 1.4282 - val_accuracy: 0.2575 - val_loss: 1.3788\nEpoch 2/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.2989 - loss: 1.3667 - val_accuracy: 0.2575 - val_loss: 1.3930\nEpoch 3/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3173 - loss: 1.3642 - val_accuracy: 0.3060 - val_loss: 1.3756\nEpoch 4/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3004 - loss: 1.3711 - val_accuracy: 0.2575 - val_loss: 1.3937\nEpoch 5/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3116 - loss: 1.3601 - val_accuracy: 0.2575 - val_loss: 1.3833\nEpoch 6/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3298 - loss: 1.3590 - val_accuracy: 0.3060 - val_loss: 1.3779\nEpoch 7/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2908 - loss: 1.3721 - val_accuracy: 0.2575 - val_loss: 1.3977\nEpoch 8/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3114 - loss: 1.3735 - val_accuracy: 0.2575 - val_loss: 1.3839\nEpoch 9/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3054 - loss: 1.3634 - val_accuracy: 0.2575 - val_loss: 1.3869\nEpoch 10/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.2888 - loss: 1.3677 - val_accuracy: 0.3060 - val_loss: 1.3742\nEpoch 11/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3016 - loss: 1.3699 - val_accuracy: 0.2575 - val_loss: 1.3956\nEpoch 12/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2943 - loss: 1.3704 - val_accuracy: 0.3060 - val_loss: 1.3783\nEpoch 13/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3008 - loss: 1.3628 - val_accuracy: 0.2575 - val_loss: 1.3817\nEpoch 14/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3165 - loss: 1.3584 - val_accuracy: 0.2575 - val_loss: 1.3904\nEpoch 15/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3058 - loss: 1.3735 - val_accuracy: 0.2575 - val_loss: 1.3839\nEpoch 16/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3169 - loss: 1.3568 - val_accuracy: 0.2575 - val_loss: 1.3794\nEpoch 17/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3033 - loss: 1.3672 - val_accuracy: 0.2575 - val_loss: 1.3870\nEpoch 18/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.3052 - loss: 1.3682 - val_accuracy: 0.2575 - val_loss: 1.3874\nEpoch 19/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3250 - loss: 1.3622 - val_accuracy: 0.2575 - val_loss: 1.3813\nEpoch 20/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2925 - loss: 1.3707 - val_accuracy: 0.2575 - val_loss: 1.3802\nEpoch 21/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3011 - loss: 1.3591 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 22/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2941 - loss: 1.3706 - val_accuracy: 0.2575 - val_loss: 1.3918\nEpoch 23/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3070 - loss: 1.3615 - val_accuracy: 0.2575 - val_loss: 1.3750\nEpoch 24/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2922 - loss: 1.3745 - val_accuracy: 0.2575 - val_loss: 1.3966\nEpoch 25/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.3191 - loss: 1.3647 - val_accuracy: 0.3060 - val_loss: 1.3824\nEpoch 26/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2727 - loss: 1.3748 - val_accuracy: 0.2575 - val_loss: 1.3859\nEpoch 27/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2974 - loss: 1.3606 - val_accuracy: 0.2575 - val_loss: 1.3813\nEpoch 28/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3017 - loss: 1.3683 - val_accuracy: 0.2575 - val_loss: 1.3866\nEpoch 29/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3133 - loss: 1.3618 - val_accuracy: 0.2575 - val_loss: 1.3818\nEpoch 30/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3032 - loss: 1.3633 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 31/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3077 - loss: 1.3688 - val_accuracy: 0.3060 - val_loss: 1.3771\nEpoch 32/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3233 - loss: 1.3594 - val_accuracy: 0.3060 - val_loss: 1.3763\nEpoch 33/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.3130 - loss: 1.3579 - val_accuracy: 0.2575 - val_loss: 1.3802\nEpoch 34/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3134 - loss: 1.3623 - val_accuracy: 0.2575 - val_loss: 1.3777\nEpoch 35/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2997 - loss: 1.3698 - val_accuracy: 0.3060 - val_loss: 1.3771\nEpoch 36/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3094 - loss: 1.3673 - val_accuracy: 0.2575 - val_loss: 1.3822\nEpoch 37/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3216 - loss: 1.3627 - val_accuracy: 0.2575 - val_loss: 1.3824\nEpoch 38/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3129 - loss: 1.3641 - val_accuracy: 0.2575 - val_loss: 1.3892\nEpoch 39/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3034 - loss: 1.3593 - val_accuracy: 0.2575 - val_loss: 1.3894\nEpoch 40/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3152 - loss: 1.3630 - val_accuracy: 0.2575 - val_loss: 1.4140\nEpoch 41/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.2941 - loss: 1.3802 - val_accuracy: 0.2575 - val_loss: 1.3939\nEpoch 42/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3027 - loss: 1.3676 - val_accuracy: 0.2575 - val_loss: 1.3793\nEpoch 43/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3079 - loss: 1.3666 - val_accuracy: 0.3060 - val_loss: 1.3768\nEpoch 44/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3168 - loss: 1.3611 - val_accuracy: 0.3060 - val_loss: 1.3795\nEpoch 45/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3073 - loss: 1.3673 - val_accuracy: 0.2575 - val_loss: 1.3847\nEpoch 46/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3056 - loss: 1.3622 - val_accuracy: 0.2575 - val_loss: 1.3819\nEpoch 47/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3160 - loss: 1.3634 - val_accuracy: 0.2575 - val_loss: 1.4097\nEpoch 48/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2862 - loss: 1.3768 - val_accuracy: 0.2575 - val_loss: 1.3757\nEpoch 49/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.2918 - loss: 1.3648 - val_accuracy: 0.2575 - val_loss: 1.3911\nEpoch 50/50\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2821 - loss: 1.3708 - val_accuracy: 0.2575 - val_loss: 1.3824\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Lab 11","metadata":{}},{"cell_type":"markdown","source":"Question A1. Continue your project work from last week’s exercise. Upon implementation of RNN for your project work, analyze the impact created on the results compared to the CNN results. ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, SimpleRNN\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:44.957066Z","iopub.execute_input":"2025-04-11T17:36:44.957591Z","iopub.status.idle":"2025-04-11T17:36:44.963994Z","shell.execute_reply.started":"2025-04-11T17:36:44.957562Z","shell.execute_reply":"2025-04-11T17:36:44.962969Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Configurations\n# ----------------------------------------\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 5  # Reduced for quick comparison\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:46.084681Z","iopub.execute_input":"2025-04-11T17:36:46.085025Z","iopub.status.idle":"2025-04-11T17:36:46.090696Z","shell.execute_reply.started":"2025-04-11T17:36:46.085001Z","shell.execute_reply":"2025-04-11T17:36:46.089866Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Data Loading (Shared for CNN & RNN)\n# ----------------------------------------\ndef load_data(flatten_for_rnn=False):\n    datagen = ImageDataGenerator(rescale=1./255)\n    train_gen = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test_gen = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n\n    if not flatten_for_rnn:\n        return train_gen, test_gen\n    \n    # For RNN - Flatten images into sequences (height, width*channels)\n    def flatten_batches(generator):\n        X, y = [], []\n        for i in range(len(generator)):\n            x_batch, y_batch = generator[i]\n            x_flat = x_batch.reshape(x_batch.shape[0], IMG_SIZE[0], IMG_SIZE[1]*3)\n            X.append(x_flat)\n            y.append(y_batch)\n            if len(X)*BATCH_SIZE >= generator.samples:\n                break\n        return np.concatenate(X), np.concatenate(y)\n\n    X_train, y_train = flatten_batches(train_gen)\n    X_test, y_test = flatten_batches(test_gen)\n    return X_train, y_train, X_test, y_test, train_gen.class_indices\n\n# ----------------------------------------\n# ✅ CNN Model\n# ----------------------------------------\ndef build_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),\n        Conv2D(32, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# ----------------------------------------\n# ✅ RNN Model\n# ----------------------------------------\ndef build_rnn_model(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),\n        SimpleRNN(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:48.524301Z","iopub.execute_input":"2025-04-11T17:36:48.524665Z","iopub.status.idle":"2025-04-11T17:36:48.536308Z","shell.execute_reply.started":"2025-04-11T17:36:48.524614Z","shell.execute_reply":"2025-04-11T17:36:48.535164Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Evaluation Helper\n# ----------------------------------------\ndef evaluate_model(model, X, y, label_map):\n    y_pred = np.argmax(model.predict(X), axis=1)\n    y_true = np.argmax(y, axis=1)\n    class_names = list(label_map.keys())\n    print(classification_report(y_true, y_pred, target_names=class_names))\n\n# ----------------------------------------\n# ✅ Run CNN\n# ----------------------------------------\ndef run_cnn():\n    print(\"\\n🚀 Running CNN...\")\n    train_data, test_data = load_data()\n    input_shape = IMG_SIZE + (3,)\n    num_classes = train_data.num_classes\n    model = build_cnn_model(input_shape, num_classes)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(train_data, epochs=EPOCHS, validation_data=test_data, verbose=1)\n\n    # Evaluate\n    test_data.reset()  # Ensure same order\n    y_true = test_data.classes\n    y_pred = np.argmax(model.predict(test_data), axis=1)\n    print(\"\\n📊 CNN Classification Report:\\n\")\n    print(classification_report(y_true, y_pred, target_names=list(test_data.class_indices.keys())))\n\n# ----------------------------------------\n# ✅ Run RNN\n# ----------------------------------------\ndef run_rnn():\n    print(\"\\n🚀 Running RNN...\")\n    X_train, y_train, X_test, y_test, label_map = load_data(flatten_for_rnn=True)\n    input_shape = (IMG_SIZE[0], IMG_SIZE[1]*3)\n    num_classes = y_train.shape[1]\n\n    model = build_rnn_model(input_shape, num_classes)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1)\n\n    # Evaluate\n    print(\"\\n📊 RNN Classification Report:\\n\")\n    evaluate_model(model, X_test, y_test, label_map)\n\n# ----------------------------------------\n# ✅ Main\n# ----------------------------------------\nif __name__ == \"__main__\":\n    run_cnn()\n    run_rnn()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:52.240936Z","iopub.execute_input":"2025-04-11T17:36:52.241251Z","iopub.status.idle":"2025-04-11T17:54:52.862599Z","shell.execute_reply.started":"2025-04-11T17:36:52.241229Z","shell.execute_reply":"2025-04-11T17:54:52.861324Z"}},"outputs":[{"name":"stdout","text":"\n🚀 Running CNN...\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.3028 - loss: 2.0410 - val_accuracy: 0.2575 - val_loss: 1.3792\nEpoch 2/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 2s/step - accuracy: 0.3142 - loss: 1.3685 - val_accuracy: 0.3631 - val_loss: 1.3310\nEpoch 3/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.5165 - loss: 1.1133 - val_accuracy: 0.8718 - val_loss: 0.4615\nEpoch 4/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 2s/step - accuracy: 0.8323 - loss: 0.4796 - val_accuracy: 0.9655 - val_loss: 0.1460\nEpoch 5/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9258 - loss: 0.2118 - val_accuracy: 0.9946 - val_loss: 0.0359\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step\n\n📊 CNN Classification Report:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.98      0.99      0.99       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      0.99      0.99       233\n                          Normal Person ECG Images (284x12=3408)       0.99      1.00      1.00       284\n\n                                                        accuracy                           0.99       928\n                                                       macro avg       0.99      0.99      0.99       928\n                                                    weighted avg       0.99      0.99      0.99       928\n\n\n🚀 Running RNN...\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.2943 - loss: 1.4563 - val_accuracy: 0.2575 - val_loss: 1.3844\nEpoch 2/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3089 - loss: 1.3653 - val_accuracy: 0.2575 - val_loss: 1.3820\nEpoch 3/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3239 - loss: 1.3598 - val_accuracy: 0.2575 - val_loss: 1.3808\nEpoch 4/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.3272 - loss: 1.3602 - val_accuracy: 0.2575 - val_loss: 1.3805\nEpoch 5/5\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3390 - loss: 1.3578 - val_accuracy: 0.3060 - val_loss: 1.3737\n\n📊 RNN Classification Report:\n\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.00      0.00      0.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.31      1.00      0.47       284\n\n                                                        accuracy                           0.31       928\n                                                       macro avg       0.08      0.25      0.12       928\n                                                    weighted avg       0.09      0.31      0.14       928\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Lab 12","metadata":{}},{"cell_type":"markdown","source":"Question A1. Implement a RNN and bi-directional RNN on your project. Additionally, implement LSTM on your project. ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, SimpleRNN, LSTM, Bidirectional, Dense\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:16:02.507834Z","iopub.execute_input":"2025-04-11T18:16:02.508474Z","iopub.status.idle":"2025-04-11T18:16:02.517031Z","shell.execute_reply.started":"2025-04-11T18:16:02.508428Z","shell.execute_reply":"2025-04-11T18:16:02.515802Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Environment Setup (CPU only + clean logs)\n# ----------------------------------------\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   # Suppress TensorFlow INFO/WARN/ERROR\n\n# ----------------------------------------\n# ✅ Configuration\n# ----------------------------------------\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 15\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:02.066615Z","iopub.execute_input":"2025-04-11T18:21:02.067014Z","iopub.status.idle":"2025-04-11T18:21:02.073290Z","shell.execute_reply.started":"2025-04-11T18:21:02.066988Z","shell.execute_reply":"2025-04-11T18:21:02.072166Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Load and Flatten Images into Sequences\n# ----------------------------------------\ndef load_sequence_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    train_gen = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test_gen = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n\n    def flatten_batches(gen):\n        X, y = [], []\n        for i in range(len(gen)):\n            x_batch, y_batch = gen[i]\n            x_flat = x_batch.reshape(x_batch.shape[0], IMG_SIZE[0], IMG_SIZE[1] * 3)  # Flatten image into sequence\n            X.append(x_flat)\n            y.append(y_batch)\n            if len(X) * BATCH_SIZE >= gen.samples:\n                break\n        return np.concatenate(X), np.concatenate(y)\n\n    X_train, y_train = flatten_batches(train_gen)\n    X_test, y_test = flatten_batches(test_gen)\n    return X_train, y_train, X_test, y_test, train_gen.class_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:03.535917Z","iopub.execute_input":"2025-04-11T18:21:03.536255Z","iopub.status.idle":"2025-04-11T18:21:03.544774Z","shell.execute_reply.started":"2025-04-11T18:21:03.536231Z","shell.execute_reply":"2025-04-11T18:21:03.543558Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, SimpleRNN, LSTM, Bidirectional, Dense\n\n# ----------------------------------------\n# ✅ RNN Variants\n# ----------------------------------------\n\n# ✅ Basic RNN\ndef build_rnn(input_shape, num_classes):\n    return Sequential([\n        Input(shape=input_shape),             # Expects (180, 540)\n        SimpleRNN(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])\n\n# ✅ Bidirectional RNN\ndef build_birnn(input_shape, num_classes):\n    return Sequential([\n        Input(shape=input_shape),             \n        Bidirectional(SimpleRNN(64, activation='tanh')),\n        Dense(num_classes, activation='softmax')\n    ])\n\n# ✅ LSTM\ndef build_lstm(input_shape, num_classes):\n    return Sequential([\n        Input(shape=input_shape),             \n        LSTM(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:07.779351Z","iopub.execute_input":"2025-04-11T18:21:07.779943Z","iopub.status.idle":"2025-04-11T18:21:07.790998Z","shell.execute_reply.started":"2025-04-11T18:21:07.779904Z","shell.execute_reply":"2025-04-11T18:21:07.789287Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ----------------------------------------\n# ✅ Train and Evaluate\n# ----------------------------------------\ndef train_and_evaluate(name, model_fn, X_train, y_train, X_test, y_test, label_map):\n    print(f\"\\n🚀 Training: {name}...\\n\")\n    model = model_fn((IMG_SIZE[0], IMG_SIZE[1] * 3), y_train.shape[1])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1)\n\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    y_true = np.argmax(y_test, axis=1)\n\n    print(f\"\\n📊 Classification Report for {name}:\\n\")\n    print(classification_report(y_true, y_pred, target_names=list(label_map.keys()), zero_division=0))\n\n# ----------------------------------------\n# ✅ Main\n# ----------------------------------------\nif __name__ == \"__main__\":\n    X_train, y_train, X_test, y_test, label_map = load_sequence_data()\n\n    train_and_evaluate(\"Simple RNN\", build_rnn, X_train, y_train, X_test, y_test, label_map)\n    train_and_evaluate(\"Bi-directional RNN\", build_birnn, X_train, y_train, X_test, y_test, label_map)\n    train_and_evaluate(\"LSTM\", build_lstm, X_train, y_train, X_test, y_test, label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:19.763232Z","iopub.execute_input":"2025-04-11T18:21:19.763613Z","iopub.status.idle":"2025-04-11T18:29:22.705020Z","shell.execute_reply.started":"2025-04-11T18:21:19.763589Z","shell.execute_reply":"2025-04-11T18:29:22.703969Z"}},"outputs":[{"name":"stdout","text":"Found 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\n\n🚀 Training: Simple RNN...\n\nEpoch 1/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.2967 - loss: 1.4241 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 2/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3206 - loss: 1.3643 - val_accuracy: 0.3060 - val_loss: 1.3814\nEpoch 3/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2951 - loss: 1.3646 - val_accuracy: 0.2575 - val_loss: 1.3899\nEpoch 4/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3021 - loss: 1.3655 - val_accuracy: 0.2575 - val_loss: 1.3877\nEpoch 5/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3102 - loss: 1.3641 - val_accuracy: 0.2575 - val_loss: 1.3770\nEpoch 6/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3115 - loss: 1.3677 - val_accuracy: 0.3060 - val_loss: 1.3774\nEpoch 7/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3098 - loss: 1.3716 - val_accuracy: 0.2575 - val_loss: 1.3968\nEpoch 8/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3137 - loss: 1.3732 - val_accuracy: 0.2575 - val_loss: 1.3788\nEpoch 9/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.3171 - loss: 1.3572 - val_accuracy: 0.2575 - val_loss: 1.3822\nEpoch 10/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3011 - loss: 1.3684 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 11/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2949 - loss: 1.3668 - val_accuracy: 0.2575 - val_loss: 1.3802\nEpoch 12/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3187 - loss: 1.3649 - val_accuracy: 0.2575 - val_loss: 1.3750\nEpoch 13/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3201 - loss: 1.3633 - val_accuracy: 0.3060 - val_loss: 1.3770\nEpoch 14/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2861 - loss: 1.3714 - val_accuracy: 0.2575 - val_loss: 1.4049\nEpoch 15/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2930 - loss: 1.3744 - val_accuracy: 0.2575 - val_loss: 1.3950\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n\n📊 Classification Report for Simple RNN:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n\n🚀 Training: Bi-directional RNN...\n\nEpoch 1/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 79ms/step - accuracy: 0.2998 - loss: 1.5083 - val_accuracy: 0.2575 - val_loss: 1.3749\nEpoch 2/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.2840 - loss: 1.3696 - val_accuracy: 0.2575 - val_loss: 1.3815\nEpoch 3/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.2985 - loss: 1.3696 - val_accuracy: 0.2575 - val_loss: 1.4082\nEpoch 4/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.2954 - loss: 1.3717 - val_accuracy: 0.2575 - val_loss: 1.3910\nEpoch 5/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.3121 - loss: 1.3692 - val_accuracy: 0.3060 - val_loss: 1.3745\nEpoch 6/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.3109 - loss: 1.3686 - val_accuracy: 0.2575 - val_loss: 1.3823\nEpoch 7/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - accuracy: 0.2993 - loss: 1.3693 - val_accuracy: 0.3060 - val_loss: 1.3776\nEpoch 8/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.2901 - loss: 1.3686 - val_accuracy: 0.2575 - val_loss: 1.4161\nEpoch 9/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - accuracy: 0.3067 - loss: 1.3755 - val_accuracy: 0.2575 - val_loss: 1.3907\nEpoch 10/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.2977 - loss: 1.3685 - val_accuracy: 0.2575 - val_loss: 1.3816\nEpoch 11/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.2816 - loss: 1.3726 - val_accuracy: 0.2575 - val_loss: 1.3963\nEpoch 12/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.2971 - loss: 1.3745 - val_accuracy: 0.2511 - val_loss: 1.4258\nEpoch 13/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.2640 - loss: 1.3822 - val_accuracy: 0.2575 - val_loss: 1.3792\nEpoch 14/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.2878 - loss: 1.3718 - val_accuracy: 0.2575 - val_loss: 1.4047\nEpoch 15/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.2970 - loss: 1.3687 - val_accuracy: 0.2575 - val_loss: 1.3993\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n\n📊 Classification Report for Bi-directional RNN:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n\n🚀 Training: LSTM...\n\nEpoch 1/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 146ms/step - accuracy: 0.3029 - loss: 1.3996 - val_accuracy: 0.2575 - val_loss: 1.3913\nEpoch 2/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 130ms/step - accuracy: 0.3057 - loss: 1.3631 - val_accuracy: 0.2575 - val_loss: 1.3835\nEpoch 3/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.3196 - loss: 1.3600 - val_accuracy: 0.2575 - val_loss: 1.3769\nEpoch 4/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 131ms/step - accuracy: 0.3006 - loss: 1.3594 - val_accuracy: 0.2575 - val_loss: 1.3796\nEpoch 5/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.3169 - loss: 1.3610 - val_accuracy: 0.2575 - val_loss: 1.3855\nEpoch 6/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.3124 - loss: 1.3618 - val_accuracy: 0.2575 - val_loss: 1.3787\nEpoch 7/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 129ms/step - accuracy: 0.3139 - loss: 1.3672 - val_accuracy: 0.2575 - val_loss: 1.3792\nEpoch 8/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.3118 - loss: 1.3610 - val_accuracy: 0.2575 - val_loss: 1.3874\nEpoch 9/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.3092 - loss: 1.3618 - val_accuracy: 0.2575 - val_loss: 1.3759\nEpoch 10/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 130ms/step - accuracy: 0.3122 - loss: 1.3598 - val_accuracy: 0.2575 - val_loss: 1.3864\nEpoch 11/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 138ms/step - accuracy: 0.3209 - loss: 1.3610 - val_accuracy: 0.2575 - val_loss: 1.3761\nEpoch 12/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 130ms/step - accuracy: 0.3129 - loss: 1.3659 - val_accuracy: 0.2575 - val_loss: 1.3788\nEpoch 13/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 140ms/step - accuracy: 0.3150 - loss: 1.3614 - val_accuracy: 0.2575 - val_loss: 1.3770\nEpoch 14/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.3112 - loss: 1.3688 - val_accuracy: 0.2575 - val_loss: 1.3780\nEpoch 15/15\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 143ms/step - accuracy: 0.3052 - loss: 1.3671 - val_accuracy: 0.2575 - val_loss: 1.3828\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n\n📊 Classification Report for LSTM:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n","output_type":"stream"}],"execution_count":28}]}