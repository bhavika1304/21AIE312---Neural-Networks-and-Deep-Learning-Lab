{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9267686,"sourceType":"datasetVersion","datasetId":5607234}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lab 9","metadata":{}},{"cell_type":"markdown","source":"Question A1. Continue your project work from last weekâ€™s exercise. Experiment with various optimizers and report your results with loss values, accuracies, precisions, recalls and F-scores.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:19.147744Z","iopub.execute_input":"2025-04-11T13:26:19.148137Z","iopub.status.idle":"2025-04-11T13:26:38.600189Z","shell.execute_reply.started":"2025-04-11T13:26:19.148110Z","shell.execute_reply":"2025-04-11T13:26:38.599139Z"}},"outputs":[{"name":"stderr","text":"2025-04-11 13:26:21.200596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744377981.477242      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744377981.551166      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force TensorFlow to use CPU only\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   # Suppress TensorFlow warnings and errors\n\n# ----------------------------------------\n# âœ… Paths and Constants\n# ----------------------------------------\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 10\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.601707Z","iopub.execute_input":"2025-04-11T13:26:38.602256Z","iopub.status.idle":"2025-04-11T13:26:38.608826Z","shell.execute_reply.started":"2025-04-11T13:26:38.602232Z","shell.execute_reply":"2025-04-11T13:26:38.607380Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Load Data using ImageDataGenerator\n# ----------------------------------------\ndef load_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    \n    train_data = datagen.flow_from_directory(\n        train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n        class_mode='categorical', seed=SEED)\n\n    test_data = datagen.flow_from_directory(\n        test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n        class_mode='categorical', shuffle=False, seed=SEED)\n    \n    return train_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.610165Z","iopub.execute_input":"2025-04-11T13:26:38.610613Z","iopub.status.idle":"2025-04-11T13:26:38.656159Z","shell.execute_reply.started":"2025-04-11T13:26:38.610579Z","shell.execute_reply":"2025-04-11T13:26:38.655080Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Define CNN Model Architecture\n# ----------------------------------------\ndef build_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),  # Add this to fix warning\n        Conv2D(32, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.658564Z","iopub.execute_input":"2025-04-11T13:26:38.658907Z","iopub.status.idle":"2025-04-11T13:26:38.679692Z","shell.execute_reply.started":"2025-04-11T13:26:38.658862Z","shell.execute_reply":"2025-04-11T13:26:38.678668Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Train Model with Given Optimizer\n# ----------------------------------------\ndef train_with_optimizer(optimizer_name, train_data, test_data):\n    print(f\"\\nğŸ”§ Training with optimizer: {optimizer_name.upper()}\\n\")\n\n    input_shape = IMG_SIZE + (3,)\n    num_classes = train_data.num_classes\n    model = build_cnn_model(input_shape, num_classes)\n\n    # Select optimizer\n    optimizer_dict = {\n        'adam': tf.keras.optimizers.Adam(),\n        'sgd': tf.keras.optimizers.SGD(),\n        'rmsprop': tf.keras.optimizers.RMSprop()\n    }\n\n    model.compile(optimizer=optimizer_dict[optimizer_name],\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    # Train model\n    history = model.fit(train_data, epochs=EPOCHS,\n                        validation_data=test_data, verbose=1)\n\n    # Predict and evaluate\n    predictions = model.predict(test_data)\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = test_data.classes\n    class_labels = list(test_data.class_indices.keys())\n\n    # Print classification report\n    print(classification_report(y_true, y_pred, target_names=class_labels))\n\n    return history\n\n# ----------------------------------------\n# âœ… Main Execution: Try All Optimizers\n# ----------------------------------------\nif __name__ == \"__main__\":\n    train_data, test_data = load_data()\n    for optimizer in ['adam', 'sgd', 'rmsprop']:\n        train_with_optimizer(optimizer, train_data, test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:26:38.680715Z","iopub.execute_input":"2025-04-11T13:26:38.680994Z","iopub.status.idle":"2025-04-11T14:56:46.593103Z","shell.execute_reply.started":"2025-04-11T13:26:38.680973Z","shell.execute_reply":"2025-04-11T14:56:46.592087Z"}},"outputs":[{"name":"stdout","text":"Found 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\n\nğŸ”§ Training with optimizer: ADAM\n\n","output_type":"stream"},{"name":"stderr","text":"2025-04-11 13:26:41.352395: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 2s/step - accuracy: 0.3575 - loss: 2.2042 - val_accuracy: 0.8955 - val_loss: 0.4036\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.8412 - loss: 0.4602 - val_accuracy: 0.9763 - val_loss: 0.1384\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9274 - loss: 0.2156 - val_accuracy: 0.9914 - val_loss: 0.0531\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9446 - loss: 0.1553 - val_accuracy: 0.9989 - val_loss: 0.0139\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 2s/step - accuracy: 0.9781 - loss: 0.0650 - val_accuracy: 1.0000 - val_loss: 0.0067\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9782 - loss: 0.0564 - val_accuracy: 1.0000 - val_loss: 0.0019\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9895 - loss: 0.0318 - val_accuracy: 1.0000 - val_loss: 0.0035\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9894 - loss: 0.0423 - val_accuracy: 1.0000 - val_loss: 0.0049\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9868 - loss: 0.0402 - val_accuracy: 1.0000 - val_loss: 0.0024\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9926 - loss: 0.0328 - val_accuracy: 1.0000 - val_loss: 7.0992e-04\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 984ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n\nğŸ”§ Training with optimizer: SGD\n\nEpoch 1/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 0.2823 - loss: 1.4560 - val_accuracy: 0.3060 - val_loss: 1.3713\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.3191 - loss: 1.3639 - val_accuracy: 0.3060 - val_loss: 1.3632\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.3297 - loss: 1.3578 - val_accuracy: 0.2748 - val_loss: 1.3479\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.3926 - loss: 1.3354 - val_accuracy: 0.7608 - val_loss: 1.2911\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.4616 - loss: 1.2710 - val_accuracy: 0.6261 - val_loss: 1.1449\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.5649 - loss: 1.1277 - val_accuracy: 0.4892 - val_loss: 1.0628\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.6784 - loss: 0.8891 - val_accuracy: 0.7295 - val_loss: 0.7851\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.7443 - loss: 0.7260 - val_accuracy: 0.8513 - val_loss: 0.5288\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.8189 - loss: 0.5376 - val_accuracy: 0.8976 - val_loss: 0.4278\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.8291 - loss: 0.4696 - val_accuracy: 0.9256 - val_loss: 0.2503\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.91      0.85      0.88       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.99      0.82      0.89       233\n                          Normal Person ECG Images (284x12=3408)       0.85      1.00      0.92       284\n\n                                                        accuracy                           0.93       928\n                                                       macro avg       0.94      0.92      0.92       928\n                                                    weighted avg       0.93      0.93      0.92       928\n\n\nğŸ”§ Training with optimizer: RMSPROP\n\nEpoch 1/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.3637 - loss: 4.2585 - val_accuracy: 0.2575 - val_loss: 2.4308\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.6776 - loss: 0.8934 - val_accuracy: 0.9817 - val_loss: 0.1185\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.8957 - loss: 0.2804 - val_accuracy: 0.9946 - val_loss: 0.0326\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.9673 - loss: 0.0981 - val_accuracy: 1.0000 - val_loss: 0.0137\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9842 - loss: 0.0483 - val_accuracy: 1.0000 - val_loss: 0.0013\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.0382 - val_accuracy: 1.0000 - val_loss: 7.1439e-04\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.9956 - loss: 0.0161 - val_accuracy: 1.0000 - val_loss: 0.0015\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.0361 - val_accuracy: 1.0000 - val_loss: 4.3497e-05\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.9963 - loss: 0.0100 - val_accuracy: 1.0000 - val_loss: 2.9650e-05\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 2s/step - accuracy: 0.9967 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 2.3899e-06\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Lab 10","metadata":{}},{"cell_type":"markdown","source":"Question A1. Continue your project work from last weekâ€™s exercise. Experiment with various regularization & optimization techniques and report your results with loss values, accuracies, precisions, recalls and F-scores.  ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.594130Z","iopub.execute_input":"2025-04-11T14:56:46.594491Z","iopub.status.idle":"2025-04-11T14:56:46.604544Z","shell.execute_reply.started":"2025-04-11T14:56:46.594463Z","shell.execute_reply":"2025-04-11T14:56:46.603374Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Environment setup\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Constants\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.605705Z","iopub.execute_input":"2025-04-11T14:56:46.606078Z","iopub.status.idle":"2025-04-11T14:56:46.627450Z","shell.execute_reply.started":"2025-04-11T14:56:46.606045Z","shell.execute_reply":"2025-04-11T14:56:46.626274Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load data\ndef load_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    train = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n    return train, test\n\n# CNN with L2 and Dropout\ndef build_regularized_cnn(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),\n        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.628550Z","iopub.execute_input":"2025-04-11T14:56:46.629277Z","iopub.status.idle":"2025-04-11T14:56:46.658058Z","shell.execute_reply.started":"2025-04-11T14:56:46.629240Z","shell.execute_reply":"2025-04-11T14:56:46.656947Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Train & Evaluate\ndef train_regularized_model(optimizer_name):\n    train_data, test_data = load_data()\n    input_shape = IMG_SIZE + (3,)\n    num_classes = train_data.num_classes\n    model = build_regularized_cnn(input_shape, num_classes)\n\n    optimizers = {\n        'adam': tf.keras.optimizers.Adam(),\n        'rmsprop': tf.keras.optimizers.RMSprop()\n    }\n\n    model.compile(optimizer=optimizers[optimizer_name],\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    model.fit(train_data, epochs=EPOCHS, validation_data=test_data, verbose=1)\n    \n    predictions = model.predict(test_data)\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = test_data.classes\n    class_labels = list(test_data.class_indices.keys())\n\n    print(classification_report(y_true, y_pred, target_names=class_labels))\n\n# Run\nif __name__ == '__main__':\n    for opt in ['adam', 'rmsprop']:\n        print(f\"\\nğŸ”§ Training with {opt.upper()} (with L2 & Dropout)\\n\")\n        train_regularized_model(opt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:56:46.659160Z","iopub.execute_input":"2025-04-11T14:56:46.660833Z","iopub.status.idle":"2025-04-11T15:59:33.975270Z","shell.execute_reply.started":"2025-04-11T14:56:46.660795Z","shell.execute_reply":"2025-04-11T15:59:33.974194Z"}},"outputs":[{"name":"stdout","text":"\nğŸ”§ Training with ADAM (with L2 & Dropout)\n\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 2s/step - accuracy: 0.2913 - loss: 2.7330 - val_accuracy: 0.3782 - val_loss: 1.4806\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.4564 - loss: 1.3592 - val_accuracy: 0.8244 - val_loss: 0.7951\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 2s/step - accuracy: 0.7417 - loss: 0.7851 - val_accuracy: 0.9310 - val_loss: 0.4356\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 2s/step - accuracy: 0.8324 - loss: 0.5690 - val_accuracy: 0.9741 - val_loss: 0.2437\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 2s/step - accuracy: 0.8621 - loss: 0.4460 - val_accuracy: 0.9881 - val_loss: 0.2119\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9018 - loss: 0.3836 - val_accuracy: 0.9935 - val_loss: 0.1763\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.9157 - loss: 0.3321 - val_accuracy: 0.9968 - val_loss: 0.1441\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.9192 - loss: 0.3084 - val_accuracy: 0.9849 - val_loss: 0.1518\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9311 - loss: 0.2846 - val_accuracy: 0.9978 - val_loss: 0.1317\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.9405 - loss: 0.2725 - val_accuracy: 1.0000 - val_loss: 0.1188\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 988ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n\nğŸ”§ Training with RMSPROP (with L2 & Dropout)\n\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2s/step - accuracy: 0.3149 - loss: 4.1873 - val_accuracy: 0.5162 - val_loss: 1.3797\nEpoch 2/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 2s/step - accuracy: 0.5657 - loss: 1.2095 - val_accuracy: 0.8567 - val_loss: 0.5850\nEpoch 3/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.8356 - loss: 0.5794 - val_accuracy: 0.9267 - val_loss: 0.3315\nEpoch 4/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9107 - loss: 0.3412 - val_accuracy: 0.9871 - val_loss: 0.2289\nEpoch 5/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.9660 - loss: 0.2253 - val_accuracy: 0.9989 - val_loss: 0.1349\nEpoch 6/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 2s/step - accuracy: 0.9733 - loss: 0.1948 - val_accuracy: 0.9440 - val_loss: 0.2413\nEpoch 7/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 2s/step - accuracy: 0.9788 - loss: 0.1722 - val_accuracy: 0.9644 - val_loss: 0.1799\nEpoch 8/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 2s/step - accuracy: 0.9785 - loss: 0.1420 - val_accuracy: 1.0000 - val_loss: 0.0907\nEpoch 9/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 2s/step - accuracy: 0.9912 - loss: 0.1128 - val_accuracy: 1.0000 - val_loss: 0.0843\nEpoch 10/10\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 2s/step - accuracy: 0.9939 - loss: 0.1068 - val_accuracy: 1.0000 - val_loss: 0.0786\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       1.00      1.00      1.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      1.00      1.00       233\n                          Normal Person ECG Images (284x12=3408)       1.00      1.00      1.00       284\n\n                                                        accuracy                           1.00       928\n                                                       macro avg       1.00      1.00      1.00       928\n                                                    weighted avg       1.00      1.00      1.00       928\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Question A2. Study about RNN and learn to implement a basic RNN. Identify how to use RNN for your project work. ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Dense, Reshape, Input\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T16:47:49.194407Z","iopub.execute_input":"2025-04-11T16:47:49.194777Z","iopub.status.idle":"2025-04-11T16:48:07.348812Z","shell.execute_reply.started":"2025-04-11T16:47:49.194753Z","shell.execute_reply":"2025-04-11T16:48:07.347774Z"}},"outputs":[{"name":"stderr","text":"2025-04-11 16:47:51.054891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744390071.317355      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744390071.391210      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Environment config\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Config\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:13:50.579991Z","iopub.execute_input":"2025-04-11T17:13:50.580335Z","iopub.status.idle":"2025-04-11T17:13:50.585734Z","shell.execute_reply.started":"2025-04-11T17:13:50.580309Z","shell.execute_reply":"2025-04-11T17:13:50.584858Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load data\ndef load_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    train = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n    return train, test\n\n# Basic RNN model (reshape image to sequences)\ndef build_rnn(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),          # âœ… input_shape = (180, 540)\n        SimpleRNN(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:13:51.699212Z","iopub.execute_input":"2025-04-11T17:13:51.699560Z","iopub.status.idle":"2025-04-11T17:13:51.706331Z","shell.execute_reply.started":"2025-04-11T17:13:51.699537Z","shell.execute_reply":"2025-04-11T17:13:51.705195Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Train and evaluate RNN\ndef train_rnn_model():\n    train_data, test_data = load_data()\n    num_classes = train_data.num_classes\n\n    # Flatten images for RNN input\n    X_train, y_train = [], []\n    for i in range(len(train_data)):\n        x, y = train_data[i]\n        X_train.append(x.reshape(x.shape[0], IMG_SIZE[0], IMG_SIZE[1]*3))\n        y_train.append(y)\n        if len(X_train)*BATCH_SIZE >= train_data.samples:\n            break\n    X_train = np.concatenate(X_train)\n    y_train = np.concatenate(y_train)\n\n    X_test, y_test = [], []\n    for i in range(len(test_data)):\n        x, y = test_data[i]\n        X_test.append(x.reshape(x.shape[0], IMG_SIZE[0], IMG_SIZE[1]*3))\n        y_test.append(y)\n        if len(X_test)*BATCH_SIZE >= test_data.samples:\n            break\n    X_test = np.concatenate(X_test)\n    y_test = np.concatenate(y_test)\n\n    model = build_rnn((IMG_SIZE[0], IMG_SIZE[1]*3), num_classes)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1)\n\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    class_labels = list(test_data.class_indices.keys())\n    print(classification_report(y_true, y_pred, target_names=class_labels))\n\n# Run\nif __name__ == '__main__':\n    train_rnn_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:13:55.115059Z","iopub.execute_input":"2025-04-11T17:13:55.115392Z","iopub.status.idle":"2025-04-11T17:19:12.307929Z","shell.execute_reply.started":"2025-04-11T17:13:55.115366Z","shell.execute_reply":"2025-04-11T17:19:12.306873Z"}},"outputs":[{"name":"stdout","text":"Found 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 51ms/step - accuracy: 0.2910 - loss: 1.4282 - val_accuracy: 0.2575 - val_loss: 1.3788\nEpoch 2/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.2989 - loss: 1.3667 - val_accuracy: 0.2575 - val_loss: 1.3930\nEpoch 3/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3173 - loss: 1.3642 - val_accuracy: 0.3060 - val_loss: 1.3756\nEpoch 4/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3004 - loss: 1.3711 - val_accuracy: 0.2575 - val_loss: 1.3937\nEpoch 5/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3116 - loss: 1.3601 - val_accuracy: 0.2575 - val_loss: 1.3833\nEpoch 6/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3298 - loss: 1.3590 - val_accuracy: 0.3060 - val_loss: 1.3779\nEpoch 7/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2908 - loss: 1.3721 - val_accuracy: 0.2575 - val_loss: 1.3977\nEpoch 8/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3114 - loss: 1.3735 - val_accuracy: 0.2575 - val_loss: 1.3839\nEpoch 9/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3054 - loss: 1.3634 - val_accuracy: 0.2575 - val_loss: 1.3869\nEpoch 10/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.2888 - loss: 1.3677 - val_accuracy: 0.3060 - val_loss: 1.3742\nEpoch 11/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3016 - loss: 1.3699 - val_accuracy: 0.2575 - val_loss: 1.3956\nEpoch 12/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2943 - loss: 1.3704 - val_accuracy: 0.3060 - val_loss: 1.3783\nEpoch 13/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3008 - loss: 1.3628 - val_accuracy: 0.2575 - val_loss: 1.3817\nEpoch 14/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3165 - loss: 1.3584 - val_accuracy: 0.2575 - val_loss: 1.3904\nEpoch 15/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3058 - loss: 1.3735 - val_accuracy: 0.2575 - val_loss: 1.3839\nEpoch 16/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3169 - loss: 1.3568 - val_accuracy: 0.2575 - val_loss: 1.3794\nEpoch 17/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3033 - loss: 1.3672 - val_accuracy: 0.2575 - val_loss: 1.3870\nEpoch 18/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.3052 - loss: 1.3682 - val_accuracy: 0.2575 - val_loss: 1.3874\nEpoch 19/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3250 - loss: 1.3622 - val_accuracy: 0.2575 - val_loss: 1.3813\nEpoch 20/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2925 - loss: 1.3707 - val_accuracy: 0.2575 - val_loss: 1.3802\nEpoch 21/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3011 - loss: 1.3591 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 22/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2941 - loss: 1.3706 - val_accuracy: 0.2575 - val_loss: 1.3918\nEpoch 23/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3070 - loss: 1.3615 - val_accuracy: 0.2575 - val_loss: 1.3750\nEpoch 24/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2922 - loss: 1.3745 - val_accuracy: 0.2575 - val_loss: 1.3966\nEpoch 25/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - accuracy: 0.3191 - loss: 1.3647 - val_accuracy: 0.3060 - val_loss: 1.3824\nEpoch 26/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2727 - loss: 1.3748 - val_accuracy: 0.2575 - val_loss: 1.3859\nEpoch 27/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2974 - loss: 1.3606 - val_accuracy: 0.2575 - val_loss: 1.3813\nEpoch 28/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3017 - loss: 1.3683 - val_accuracy: 0.2575 - val_loss: 1.3866\nEpoch 29/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3133 - loss: 1.3618 - val_accuracy: 0.2575 - val_loss: 1.3818\nEpoch 30/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3032 - loss: 1.3633 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 31/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3077 - loss: 1.3688 - val_accuracy: 0.3060 - val_loss: 1.3771\nEpoch 32/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3233 - loss: 1.3594 - val_accuracy: 0.3060 - val_loss: 1.3763\nEpoch 33/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.3130 - loss: 1.3579 - val_accuracy: 0.2575 - val_loss: 1.3802\nEpoch 34/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3134 - loss: 1.3623 - val_accuracy: 0.2575 - val_loss: 1.3777\nEpoch 35/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2997 - loss: 1.3698 - val_accuracy: 0.3060 - val_loss: 1.3771\nEpoch 36/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3094 - loss: 1.3673 - val_accuracy: 0.2575 - val_loss: 1.3822\nEpoch 37/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3216 - loss: 1.3627 - val_accuracy: 0.2575 - val_loss: 1.3824\nEpoch 38/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3129 - loss: 1.3641 - val_accuracy: 0.2575 - val_loss: 1.3892\nEpoch 39/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3034 - loss: 1.3593 - val_accuracy: 0.2575 - val_loss: 1.3894\nEpoch 40/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3152 - loss: 1.3630 - val_accuracy: 0.2575 - val_loss: 1.4140\nEpoch 41/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.2941 - loss: 1.3802 - val_accuracy: 0.2575 - val_loss: 1.3939\nEpoch 42/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3027 - loss: 1.3676 - val_accuracy: 0.2575 - val_loss: 1.3793\nEpoch 43/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3079 - loss: 1.3666 - val_accuracy: 0.3060 - val_loss: 1.3768\nEpoch 44/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3168 - loss: 1.3611 - val_accuracy: 0.3060 - val_loss: 1.3795\nEpoch 45/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3073 - loss: 1.3673 - val_accuracy: 0.2575 - val_loss: 1.3847\nEpoch 46/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3056 - loss: 1.3622 - val_accuracy: 0.2575 - val_loss: 1.3819\nEpoch 47/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3160 - loss: 1.3634 - val_accuracy: 0.2575 - val_loss: 1.4097\nEpoch 48/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2862 - loss: 1.3768 - val_accuracy: 0.2575 - val_loss: 1.3757\nEpoch 49/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.2918 - loss: 1.3648 - val_accuracy: 0.2575 - val_loss: 1.3911\nEpoch 50/50\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2821 - loss: 1.3708 - val_accuracy: 0.2575 - val_loss: 1.3824\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Lab 11","metadata":{}},{"cell_type":"markdown","source":"Question A1. Continue your project work from last weekâ€™s exercise. Upon implementation of RNN for your project work, analyze the impact created on the results compared to the CNN results. ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, SimpleRNN\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:44.957066Z","iopub.execute_input":"2025-04-11T17:36:44.957591Z","iopub.status.idle":"2025-04-11T17:36:44.963994Z","shell.execute_reply.started":"2025-04-11T17:36:44.957562Z","shell.execute_reply":"2025-04-11T17:36:44.962969Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Configurations\n# ----------------------------------------\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 5  # Reduced for quick comparison\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:46.084681Z","iopub.execute_input":"2025-04-11T17:36:46.085025Z","iopub.status.idle":"2025-04-11T17:36:46.090696Z","shell.execute_reply.started":"2025-04-11T17:36:46.085001Z","shell.execute_reply":"2025-04-11T17:36:46.089866Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Data Loading (Shared for CNN & RNN)\n# ----------------------------------------\ndef load_data(flatten_for_rnn=False):\n    datagen = ImageDataGenerator(rescale=1./255)\n    train_gen = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test_gen = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n\n    if not flatten_for_rnn:\n        return train_gen, test_gen\n    \n    # For RNN - Flatten images into sequences (height, width*channels)\n    def flatten_batches(generator):\n        X, y = [], []\n        for i in range(len(generator)):\n            x_batch, y_batch = generator[i]\n            x_flat = x_batch.reshape(x_batch.shape[0], IMG_SIZE[0], IMG_SIZE[1]*3)\n            X.append(x_flat)\n            y.append(y_batch)\n            if len(X)*BATCH_SIZE >= generator.samples:\n                break\n        return np.concatenate(X), np.concatenate(y)\n\n    X_train, y_train = flatten_batches(train_gen)\n    X_test, y_test = flatten_batches(test_gen)\n    return X_train, y_train, X_test, y_test, train_gen.class_indices\n\n# ----------------------------------------\n# âœ… CNN Model\n# ----------------------------------------\ndef build_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),\n        Conv2D(32, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# ----------------------------------------\n# âœ… RNN Model\n# ----------------------------------------\ndef build_rnn_model(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),\n        SimpleRNN(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:48.524301Z","iopub.execute_input":"2025-04-11T17:36:48.524665Z","iopub.status.idle":"2025-04-11T17:36:48.536308Z","shell.execute_reply.started":"2025-04-11T17:36:48.524614Z","shell.execute_reply":"2025-04-11T17:36:48.535164Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Evaluation Helper\n# ----------------------------------------\ndef evaluate_model(model, X, y, label_map):\n    y_pred = np.argmax(model.predict(X), axis=1)\n    y_true = np.argmax(y, axis=1)\n    class_names = list(label_map.keys())\n    print(classification_report(y_true, y_pred, target_names=class_names))\n\n# ----------------------------------------\n# âœ… Run CNN\n# ----------------------------------------\ndef run_cnn():\n    print(\"\\nğŸš€ Running CNN...\")\n    train_data, test_data = load_data()\n    input_shape = IMG_SIZE + (3,)\n    num_classes = train_data.num_classes\n    model = build_cnn_model(input_shape, num_classes)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(train_data, epochs=EPOCHS, validation_data=test_data, verbose=1)\n\n    # Evaluate\n    test_data.reset()  # Ensure same order\n    y_true = test_data.classes\n    y_pred = np.argmax(model.predict(test_data), axis=1)\n    print(\"\\nğŸ“Š CNN Classification Report:\\n\")\n    print(classification_report(y_true, y_pred, target_names=list(test_data.class_indices.keys())))\n\n# ----------------------------------------\n# âœ… Run RNN\n# ----------------------------------------\ndef run_rnn():\n    print(\"\\nğŸš€ Running RNN...\")\n    X_train, y_train, X_test, y_test, label_map = load_data(flatten_for_rnn=True)\n    input_shape = (IMG_SIZE[0], IMG_SIZE[1]*3)\n    num_classes = y_train.shape[1]\n\n    model = build_rnn_model(input_shape, num_classes)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1)\n\n    # Evaluate\n    print(\"\\nğŸ“Š RNN Classification Report:\\n\")\n    evaluate_model(model, X_test, y_test, label_map)\n\n# ----------------------------------------\n# âœ… Main\n# ----------------------------------------\nif __name__ == \"__main__\":\n    run_cnn()\n    run_rnn()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:36:52.240936Z","iopub.execute_input":"2025-04-11T17:36:52.241251Z","iopub.status.idle":"2025-04-11T17:54:52.862599Z","shell.execute_reply.started":"2025-04-11T17:36:52.241229Z","shell.execute_reply":"2025-04-11T17:54:52.861324Z"}},"outputs":[{"name":"stdout","text":"\nğŸš€ Running CNN...\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.3028 - loss: 2.0410 - val_accuracy: 0.2575 - val_loss: 1.3792\nEpoch 2/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 2s/step - accuracy: 0.3142 - loss: 1.3685 - val_accuracy: 0.3631 - val_loss: 1.3310\nEpoch 3/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.5165 - loss: 1.1133 - val_accuracy: 0.8718 - val_loss: 0.4615\nEpoch 4/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 2s/step - accuracy: 0.8323 - loss: 0.4796 - val_accuracy: 0.9655 - val_loss: 0.1460\nEpoch 5/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.9258 - loss: 0.2118 - val_accuracy: 0.9946 - val_loss: 0.0359\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step\n\nğŸ“Š CNN Classification Report:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       1.00      1.00      1.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.98      0.99      0.99       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       1.00      0.99      0.99       233\n                          Normal Person ECG Images (284x12=3408)       0.99      1.00      1.00       284\n\n                                                        accuracy                           0.99       928\n                                                       macro avg       0.99      0.99      0.99       928\n                                                    weighted avg       0.99      0.99      0.99       928\n\n\nğŸš€ Running RNN...\nFound 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\nEpoch 1/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.2943 - loss: 1.4563 - val_accuracy: 0.2575 - val_loss: 1.3844\nEpoch 2/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3089 - loss: 1.3653 - val_accuracy: 0.2575 - val_loss: 1.3820\nEpoch 3/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3239 - loss: 1.3598 - val_accuracy: 0.2575 - val_loss: 1.3808\nEpoch 4/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.3272 - loss: 1.3602 - val_accuracy: 0.2575 - val_loss: 1.3805\nEpoch 5/5\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3390 - loss: 1.3578 - val_accuracy: 0.3060 - val_loss: 1.3737\n\nğŸ“Š RNN Classification Report:\n\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.00      0.00      0.00       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.31      1.00      0.47       284\n\n                                                        accuracy                           0.31       928\n                                                       macro avg       0.08      0.25      0.12       928\n                                                    weighted avg       0.09      0.31      0.14       928\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Lab 12","metadata":{}},{"cell_type":"markdown","source":"Question A1. Implement a RNN and bi-directional RNN on your project. Additionally, implement LSTM on your project. ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, SimpleRNN, LSTM, Bidirectional, Dense\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:16:02.507834Z","iopub.execute_input":"2025-04-11T18:16:02.508474Z","iopub.status.idle":"2025-04-11T18:16:02.517031Z","shell.execute_reply.started":"2025-04-11T18:16:02.508428Z","shell.execute_reply":"2025-04-11T18:16:02.515802Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Environment Setup (CPU only + clean logs)\n# ----------------------------------------\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   # Suppress TensorFlow INFO/WARN/ERROR\n\n# ----------------------------------------\n# âœ… Configuration\n# ----------------------------------------\nIMG_SIZE = (180, 180)\nBATCH_SIZE = 32\nEPOCHS = 15\ntrain_dir = '/kaggle/input/ecg-analysis/ECG_DATA/train'\ntest_dir = '/kaggle/input/ecg-analysis/ECG_DATA/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:02.066615Z","iopub.execute_input":"2025-04-11T18:21:02.067014Z","iopub.status.idle":"2025-04-11T18:21:02.073290Z","shell.execute_reply.started":"2025-04-11T18:21:02.066988Z","shell.execute_reply":"2025-04-11T18:21:02.072166Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Load and Flatten Images into Sequences\n# ----------------------------------------\ndef load_sequence_data():\n    datagen = ImageDataGenerator(rescale=1./255)\n    train_gen = datagen.flow_from_directory(train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n    test_gen = datagen.flow_from_directory(test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n\n    def flatten_batches(gen):\n        X, y = [], []\n        for i in range(len(gen)):\n            x_batch, y_batch = gen[i]\n            x_flat = x_batch.reshape(x_batch.shape[0], IMG_SIZE[0], IMG_SIZE[1] * 3)  # Flatten image into sequence\n            X.append(x_flat)\n            y.append(y_batch)\n            if len(X) * BATCH_SIZE >= gen.samples:\n                break\n        return np.concatenate(X), np.concatenate(y)\n\n    X_train, y_train = flatten_batches(train_gen)\n    X_test, y_test = flatten_batches(test_gen)\n    return X_train, y_train, X_test, y_test, train_gen.class_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:03.535917Z","iopub.execute_input":"2025-04-11T18:21:03.536255Z","iopub.status.idle":"2025-04-11T18:21:03.544774Z","shell.execute_reply.started":"2025-04-11T18:21:03.536231Z","shell.execute_reply":"2025-04-11T18:21:03.543558Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, SimpleRNN, LSTM, Bidirectional, Dense\n\n# ----------------------------------------\n# âœ… RNN Variants\n# ----------------------------------------\n\n# âœ… Basic RNN\ndef build_rnn(input_shape, num_classes):\n    return Sequential([\n        Input(shape=input_shape),             # Expects (180, 540)\n        SimpleRNN(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])\n\n# âœ… Bidirectional RNN\ndef build_birnn(input_shape, num_classes):\n    return Sequential([\n        Input(shape=input_shape),             \n        Bidirectional(SimpleRNN(64, activation='tanh')),\n        Dense(num_classes, activation='softmax')\n    ])\n\n# âœ… LSTM\ndef build_lstm(input_shape, num_classes):\n    return Sequential([\n        Input(shape=input_shape),             \n        LSTM(64, activation='tanh'),\n        Dense(num_classes, activation='softmax')\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:07.779351Z","iopub.execute_input":"2025-04-11T18:21:07.779943Z","iopub.status.idle":"2025-04-11T18:21:07.790998Z","shell.execute_reply.started":"2025-04-11T18:21:07.779904Z","shell.execute_reply":"2025-04-11T18:21:07.789287Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ----------------------------------------\n# âœ… Train and Evaluate\n# ----------------------------------------\ndef train_and_evaluate(name, model_fn, X_train, y_train, X_test, y_test, label_map):\n    print(f\"\\nğŸš€ Training: {name}...\\n\")\n    model = model_fn((IMG_SIZE[0], IMG_SIZE[1] * 3), y_train.shape[1])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1)\n\n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    y_true = np.argmax(y_test, axis=1)\n\n    print(f\"\\nğŸ“Š Classification Report for {name}:\\n\")\n    print(classification_report(y_true, y_pred, target_names=list(label_map.keys()), zero_division=0))\n\n# ----------------------------------------\n# âœ… Main\n# ----------------------------------------\nif __name__ == \"__main__\":\n    X_train, y_train, X_test, y_test, label_map = load_sequence_data()\n\n    train_and_evaluate(\"Simple RNN\", build_rnn, X_train, y_train, X_test, y_test, label_map)\n    train_and_evaluate(\"Bi-directional RNN\", build_birnn, X_train, y_train, X_test, y_test, label_map)\n    train_and_evaluate(\"LSTM\", build_lstm, X_train, y_train, X_test, y_test, label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T18:21:19.763232Z","iopub.execute_input":"2025-04-11T18:21:19.763613Z","iopub.status.idle":"2025-04-11T18:29:22.705020Z","shell.execute_reply.started":"2025-04-11T18:21:19.763589Z","shell.execute_reply":"2025-04-11T18:29:22.703969Z"}},"outputs":[{"name":"stdout","text":"Found 3023 images belonging to 4 classes.\nFound 928 images belonging to 4 classes.\n\nğŸš€ Training: Simple RNN...\n\nEpoch 1/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.2967 - loss: 1.4241 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 2/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3206 - loss: 1.3643 - val_accuracy: 0.3060 - val_loss: 1.3814\nEpoch 3/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.2951 - loss: 1.3646 - val_accuracy: 0.2575 - val_loss: 1.3899\nEpoch 4/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3021 - loss: 1.3655 - val_accuracy: 0.2575 - val_loss: 1.3877\nEpoch 5/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3102 - loss: 1.3641 - val_accuracy: 0.2575 - val_loss: 1.3770\nEpoch 6/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3115 - loss: 1.3677 - val_accuracy: 0.3060 - val_loss: 1.3774\nEpoch 7/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3098 - loss: 1.3716 - val_accuracy: 0.2575 - val_loss: 1.3968\nEpoch 8/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.3137 - loss: 1.3732 - val_accuracy: 0.2575 - val_loss: 1.3788\nEpoch 9/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.3171 - loss: 1.3572 - val_accuracy: 0.2575 - val_loss: 1.3822\nEpoch 10/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3011 - loss: 1.3684 - val_accuracy: 0.2575 - val_loss: 1.3826\nEpoch 11/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2949 - loss: 1.3668 - val_accuracy: 0.2575 - val_loss: 1.3802\nEpoch 12/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.3187 - loss: 1.3649 - val_accuracy: 0.2575 - val_loss: 1.3750\nEpoch 13/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.3201 - loss: 1.3633 - val_accuracy: 0.3060 - val_loss: 1.3770\nEpoch 14/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2861 - loss: 1.3714 - val_accuracy: 0.2575 - val_loss: 1.4049\nEpoch 15/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.2930 - loss: 1.3744 - val_accuracy: 0.2575 - val_loss: 1.3950\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n\nğŸ“Š Classification Report for Simple RNN:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n\nğŸš€ Training: Bi-directional RNN...\n\nEpoch 1/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 79ms/step - accuracy: 0.2998 - loss: 1.5083 - val_accuracy: 0.2575 - val_loss: 1.3749\nEpoch 2/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.2840 - loss: 1.3696 - val_accuracy: 0.2575 - val_loss: 1.3815\nEpoch 3/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.2985 - loss: 1.3696 - val_accuracy: 0.2575 - val_loss: 1.4082\nEpoch 4/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.2954 - loss: 1.3717 - val_accuracy: 0.2575 - val_loss: 1.3910\nEpoch 5/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.3121 - loss: 1.3692 - val_accuracy: 0.3060 - val_loss: 1.3745\nEpoch 6/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.3109 - loss: 1.3686 - val_accuracy: 0.2575 - val_loss: 1.3823\nEpoch 7/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - accuracy: 0.2993 - loss: 1.3693 - val_accuracy: 0.3060 - val_loss: 1.3776\nEpoch 8/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.2901 - loss: 1.3686 - val_accuracy: 0.2575 - val_loss: 1.4161\nEpoch 9/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - accuracy: 0.3067 - loss: 1.3755 - val_accuracy: 0.2575 - val_loss: 1.3907\nEpoch 10/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.2977 - loss: 1.3685 - val_accuracy: 0.2575 - val_loss: 1.3816\nEpoch 11/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.2816 - loss: 1.3726 - val_accuracy: 0.2575 - val_loss: 1.3963\nEpoch 12/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.2971 - loss: 1.3745 - val_accuracy: 0.2511 - val_loss: 1.4258\nEpoch 13/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.2640 - loss: 1.3822 - val_accuracy: 0.2575 - val_loss: 1.3792\nEpoch 14/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.2878 - loss: 1.3718 - val_accuracy: 0.2575 - val_loss: 1.4047\nEpoch 15/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.2970 - loss: 1.3687 - val_accuracy: 0.2575 - val_loss: 1.3993\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n\nğŸ“Š Classification Report for Bi-directional RNN:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n\nğŸš€ Training: LSTM...\n\nEpoch 1/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 146ms/step - accuracy: 0.3029 - loss: 1.3996 - val_accuracy: 0.2575 - val_loss: 1.3913\nEpoch 2/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 130ms/step - accuracy: 0.3057 - loss: 1.3631 - val_accuracy: 0.2575 - val_loss: 1.3835\nEpoch 3/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.3196 - loss: 1.3600 - val_accuracy: 0.2575 - val_loss: 1.3769\nEpoch 4/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 131ms/step - accuracy: 0.3006 - loss: 1.3594 - val_accuracy: 0.2575 - val_loss: 1.3796\nEpoch 5/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.3169 - loss: 1.3610 - val_accuracy: 0.2575 - val_loss: 1.3855\nEpoch 6/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.3124 - loss: 1.3618 - val_accuracy: 0.2575 - val_loss: 1.3787\nEpoch 7/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 129ms/step - accuracy: 0.3139 - loss: 1.3672 - val_accuracy: 0.2575 - val_loss: 1.3792\nEpoch 8/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.3118 - loss: 1.3610 - val_accuracy: 0.2575 - val_loss: 1.3874\nEpoch 9/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.3092 - loss: 1.3618 - val_accuracy: 0.2575 - val_loss: 1.3759\nEpoch 10/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 130ms/step - accuracy: 0.3122 - loss: 1.3598 - val_accuracy: 0.2575 - val_loss: 1.3864\nEpoch 11/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 138ms/step - accuracy: 0.3209 - loss: 1.3610 - val_accuracy: 0.2575 - val_loss: 1.3761\nEpoch 12/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 130ms/step - accuracy: 0.3129 - loss: 1.3659 - val_accuracy: 0.2575 - val_loss: 1.3788\nEpoch 13/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 140ms/step - accuracy: 0.3150 - loss: 1.3614 - val_accuracy: 0.2575 - val_loss: 1.3770\nEpoch 14/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.3112 - loss: 1.3688 - val_accuracy: 0.2575 - val_loss: 1.3780\nEpoch 15/15\n\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 143ms/step - accuracy: 0.3052 - loss: 1.3671 - val_accuracy: 0.2575 - val_loss: 1.3828\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n\nğŸ“Š Classification Report for LSTM:\n\n                                                                  precision    recall  f1-score   support\n\n      ECG Images of Myocardial Infarction Patients (240x12=2880)       0.26      1.00      0.41       239\n     ECG Images of Patient that have History of MI (172x12=2064)       0.00      0.00      0.00       172\nECG Images of Patient that have abnormal heartbeat (233x12=2796)       0.00      0.00      0.00       233\n                          Normal Person ECG Images (284x12=3408)       0.00      0.00      0.00       284\n\n                                                        accuracy                           0.26       928\n                                                       macro avg       0.06      0.25      0.10       928\n                                                    weighted avg       0.07      0.26      0.11       928\n\n","output_type":"stream"}],"execution_count":28}]}